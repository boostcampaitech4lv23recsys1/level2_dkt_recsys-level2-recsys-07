{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype = {\"userID\": \"int16\", \"answerCode\": \"int8\", \"KnowledgeTag\": \"int16\"}\n",
    "\n",
    "DATA_PATH = \"/opt/ml/input/data/total_data.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, dtype=dtype, parse_dates=[\"Timestamp\"])\n",
    "df = df.sort_values(by=[\"userID\", \"Timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "train = pd.read_csv(DATA_PATH, dtype=dtype, parse_dates=[\"Timestamp\"])\n",
    "train = train.sort_values(by=[\"userID\", \"Timestamp\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELO\n",
    "def ELO_function(df):\n",
    "    def get_new_theta(is_good_answer, beta, left_asymptote, theta, nb_previous_answers):\n",
    "        return theta + learning_rate_theta(nb_previous_answers) * (\n",
    "            is_good_answer - probability_of_good_answer(theta, beta, left_asymptote)\n",
    "        )\n",
    "\n",
    "    def get_new_beta(is_good_answer, beta, left_asymptote, theta, nb_previous_answers):\n",
    "        return beta - learning_rate_beta(nb_previous_answers) * (\n",
    "            is_good_answer - probability_of_good_answer(theta, beta, left_asymptote)\n",
    "        )\n",
    "\n",
    "    def learning_rate_theta(nb_answers):\n",
    "        return max(0.3 / (1 + 0.01 * nb_answers), 0.04)\n",
    "\n",
    "    def learning_rate_beta(nb_answers):\n",
    "        return 1 / (1 + 0.05 * nb_answers)\n",
    "\n",
    "    def probability_of_good_answer(theta, beta, left_asymptote):\n",
    "        return left_asymptote + (1 - left_asymptote) * sigmoid(theta - beta)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def estimate_parameters(answers_df, granularity_feature_name=\"assessmentItemID\"):\n",
    "        item_parameters = {\n",
    "            granularity_feature_value: {\"beta\": 0, \"nb_answers\": 0}\n",
    "            for granularity_feature_value in np.unique(\n",
    "                answers_df[granularity_feature_name]\n",
    "            )\n",
    "        }\n",
    "        student_parameters = {\n",
    "            student_id: {\"theta\": 0, \"nb_answers\": 0}\n",
    "            for student_id in np.unique(answers_df.userID)\n",
    "        }\n",
    "\n",
    "        print(\"Parameter estimation is starting...\")\n",
    "\n",
    "        for student_id, item_id, left_asymptote, answered_correctly in tqdm(\n",
    "            zip(\n",
    "                answers_df.userID.values,\n",
    "                answers_df[granularity_feature_name].values,\n",
    "                answers_df.left_asymptote.values,\n",
    "                answers_df.answerCode.values,\n",
    "            )\n",
    "        ):\n",
    "            theta = student_parameters[student_id][\"theta\"]\n",
    "            beta = item_parameters[item_id][\"beta\"]\n",
    "\n",
    "            item_parameters[item_id][\"beta\"] = get_new_beta(\n",
    "                answered_correctly,\n",
    "                beta,\n",
    "                left_asymptote,\n",
    "                theta,\n",
    "                item_parameters[item_id][\"nb_answers\"],\n",
    "            )\n",
    "            student_parameters[student_id][\"theta\"] = get_new_theta(\n",
    "                answered_correctly,\n",
    "                beta,\n",
    "                left_asymptote,\n",
    "                theta,\n",
    "                student_parameters[student_id][\"nb_answers\"],\n",
    "            )\n",
    "\n",
    "            item_parameters[item_id][\"nb_answers\"] += 1\n",
    "            student_parameters[student_id][\"nb_answers\"] += 1\n",
    "\n",
    "        print(f\"Theta & beta estimations on {granularity_feature_name} are completed.\")\n",
    "        return student_parameters, item_parameters\n",
    "\n",
    "    def gou_func(theta, beta):\n",
    "        return 1 / (1 + np.exp(-(theta - beta)))\n",
    "\n",
    "    df[\"left_asymptote\"] = 0\n",
    "\n",
    "    print(f\"Dataset of shape {df.shape}\")\n",
    "    print(f\"Columns are {list(df.columns)}\")\n",
    "\n",
    "    student_parameters, item_parameters = estimate_parameters(df)\n",
    "\n",
    "    prob = [\n",
    "        gou_func(student_parameters[student][\"theta\"], item_parameters[item][\"beta\"])\n",
    "        for student, item in zip(df.userID.values, df.assessmentItemID.values)\n",
    "    ]\n",
    "\n",
    "    df[\"elo_prob\"] = prob\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset of shape (2526700, 8)\n",
      "Columns are ['userID', 'assessmentItemID', 'testId', 'answerCode', 'Timestamp', 'KnowledgeTag', 'dataset', 'left_asymptote']\n",
      "Parameter estimation is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2526700it [00:19, 132965.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta & beta estimations on assessmentItemID are completed.\n"
     ]
    }
   ],
   "source": [
    "def feature_engineering(df):\n",
    "\n",
    "    # 유저별 시퀀스를 고려하기 위해 정렬\n",
    "    df.sort_values(by=[\"userID\", \"Timestamp\"], inplace=True)\n",
    "    # ELO\n",
    "    df = ELO_function(df)\n",
    "\n",
    "    df[\"hour\"] = df[\"Timestamp\"].dt.hour  # 시간\n",
    "    df[\"dow\"] = df[\"Timestamp\"].dt.dayofweek  # 요일\n",
    "\n",
    "    # 풀이시간\n",
    "    solving_time = df[['userID', 'Timestamp']].groupby('userID').diff(periods=-1).fillna(pd.Timedelta(seconds=0))\n",
    "    solving_time = solving_time['Timestamp'].apply(lambda x: x.total_seconds())\n",
    "    df['elap_time'] = -solving_time\n",
    "    df[\"elap_time\"] = df[\"elap_time\"].apply(lambda x: x if x < 700 and x >= 0 else 0)\n",
    "\n",
    "    df[\"prefix\"] = df[\"testId\"].map(lambda x: int(x[1:4]) // 10)  # 대분류\n",
    "    df[\"mid\"] = df[\"testId\"].map(lambda x: int(x[-3:]))  # 중분류\n",
    "    df['suffix'] = df[\"assessmentItemID\"].map(lambda x: int(x[-3:])) # 소분류\n",
    "\n",
    "    correct_t = df.groupby([\"testId\"])[\"answerCode\"].agg([\"mean\", \"sum\"])\n",
    "    correct_t.columns = [\"test_mean\", \"test_sum\"]  # 시험지별 정답률\n",
    "    correct_k = df.groupby([\"KnowledgeTag\"])[\"answerCode\"].agg([\"mean\", \"sum\"])\n",
    "    correct_k.columns = [\"tag_mean\", \"tag_sum\"]  # tag별 정답률\n",
    "    correct_a = df.groupby([\"assessmentItemID\"])[\"answerCode\"].agg([\"mean\", \"sum\"])\n",
    "    correct_a.columns = [\"ass_mean\", \"ass_sum\"]  # 문제별 정답률\n",
    "    correct_h = df.groupby([\"hour\"])[\"answerCode\"].agg([\"mean\", \"sum\"]) # 시간별 정답률\n",
    "    correct_h.columns = [\"hour_mean\", \"hour_sum\"]\n",
    "    correct_d = df.groupby([\"dow\"])[\"answerCode\"].agg([\"mean\", \"sum\"])  # 요일별 정답률\n",
    "    correct_d.columns = [\"dow_mean\", \"dow_sum\"]\n",
    "    correct_s = df.groupby([\"suffix\"])[\"answerCode\"].agg([\"mean\", \"sum\"]) # 뒤의 3자리별 정답률\n",
    "    correct_s.columns = [\"suffix_mean\", \"suffix_sum\"]  \n",
    "\n",
    "    df = pd.merge(df, correct_t, on=[\"testId\"], how=\"left\")\n",
    "    df = pd.merge(df, correct_k, on=[\"KnowledgeTag\"], how=\"left\")\n",
    "    df = pd.merge(df, correct_a, on=[\"assessmentItemID\"], how=\"left\")\n",
    "    df = pd.merge(df, correct_s, on=[\"suffix\"], how=\"left\")\n",
    "    df = pd.merge(df, correct_h, on=[\"hour\"], how=\"left\")\n",
    "    df = pd.merge(df, correct_d, on=[\"dow\"], how=\"left\")\n",
    "\n",
    "    ## 여까지 왓네\n",
    "\n",
    "    df_o = df[df[\"answerCode\"] == 1]\n",
    "    df_x = df[df[\"answerCode\"] == 0]\n",
    "\n",
    "    # tag별 풀이시간 평균\n",
    "    tag_elp = df.groupby([\"KnowledgeTag\"])[\"elap_time\"].agg(\"mean\").reset_index()\n",
    "    tag_elp.columns = [\"KnowledgeTag\", \"tag_elp\"]\n",
    "    tag_elp_o = df_o.groupby([\"KnowledgeTag\"])[\"elap_time\"].agg(\"mean\").reset_index()\n",
    "    tag_elp_o.columns = [\"KnowledgeTag\", \"tag_elp_o\"]\n",
    "    tag_elp_x = df_x.groupby([\"KnowledgeTag\"])[\"elap_time\"].agg(\"mean\").reset_index()\n",
    "    tag_elp_x.columns = [\"KnowledgeTag\", \"tag_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, tag_elp, on=[\"KnowledgeTag\"], how=\"left\")\n",
    "    df = pd.merge(df, tag_elp_o, on=[\"KnowledgeTag\"], how=\"left\")\n",
    "    df = pd.merge(df, tag_elp_x, on=[\"KnowledgeTag\"], how=\"left\")\n",
    "\n",
    "    # 문제별 풀이시간 평균\n",
    "    ass_elp = df.groupby([\"assessmentItemID\"])[\"elap_time\"].agg(\"mean\").reset_index()\n",
    "    ass_elp.columns = [\"assessmentItemID\", \"ass_elp\"]\n",
    "    ass_elp_o = df_o.groupby([\"assessmentItemID\"])[\"elap_time\"].agg(\"mean\").reset_index()\n",
    "    ass_elp_o.columns = [\"assessmentItemID\", \"ass_elp_o\"]\n",
    "    ass_elp_x = df_x.groupby([\"assessmentItemID\"])[\"elap_time\"].agg(\"mean\").reset_index()\n",
    "    ass_elp_x.columns = [\"assessmentItemID\", \"ass_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, ass_elp, on=[\"assessmentItemID\"], how=\"left\")\n",
    "    df = pd.merge(df, ass_elp_o, on=[\"assessmentItemID\"], how=\"left\")\n",
    "    df = pd.merge(df, ass_elp_x, on=[\"assessmentItemID\"], how=\"left\")\n",
    "\n",
    "    # 문항번호별 풀이시간 평균\n",
    "    suffix_elp = df.groupby([\"suffix\"])[\"elap_time\"].agg(\"mean\").reset_index()\n",
    "    suffix_elp.columns = [\"suffix\", \"suffix_elp\"]\n",
    "    suffix_elp_o = df_o.groupby([\"suffix\"])[\"elap_time\"].agg(\"mean\").reset_index()\n",
    "    suffix_elp_o.columns = [\"suffix\", \"suffix_elp_o\"]\n",
    "    suffix_elp_x = df_x.groupby([\"suffix\"])[\"elap_time\"].agg(\"mean\").reset_index()\n",
    "    suffix_elp_x.columns = [\"suffix\", \"suffix_elp_x\"]\n",
    "\n",
    "    df = pd.merge(df, suffix_elp, on=[\"suffix\"], how=\"left\")\n",
    "    df = pd.merge(df, suffix_elp_o, on=[\"suffix\"], how=\"left\")\n",
    "    df = pd.merge(df, suffix_elp_x, on=[\"suffix\"], how=\"left\")\n",
    "\n",
    "    df[\"user_correct_answer\"] = (df.groupby(\"userID\")[\"answerCode\"].transform(lambda x: x.cumsum().shift(1))\n",
    "        .fillna(0)\n",
    "    )\n",
    "    df[\"user_total_answer\"] = df.groupby(\"userID\")[\"answerCode\"].cumcount() # 푼 문제수\n",
    "    df[\"user_acc\"] = (df[\"user_correct_answer\"] / df[\"user_total_answer\"]).fillna(0) # 정답률\n",
    "\n",
    "    df[\"Prefix_o\"] = (\n",
    "        df.groupby([\"userID\", \"prefix\"])[\"answerCode\"]\n",
    "        .transform(lambda x: x.cumsum().shift(1))\n",
    "        .fillna(0)\n",
    "    )\n",
    "    # 문항별로 분류\n",
    "    df[\"PrefixCount\"] = df.groupby([\"userID\", \"prefix\"]).cumcount()\n",
    "    df[\"PrefixAcc\"] = (df[\"Prefix_o\"] / df[\"PrefixCount\"]).fillna(0)\n",
    "    df[\"PrefixElp\"] = (\n",
    "        df.groupby([\"userID\", \"prefix\"])[\"elap_time\"]\n",
    "        .transform(lambda x: x.cumsum())\n",
    "        .fillna(0)\n",
    "    )\n",
    "    df[\"PrefixMElp\"] = df[\"PrefixElp\"] / [\n",
    "        v if v != 0 else 1 for v in df[\"PrefixCount\"].values\n",
    "    ]\n",
    "\n",
    "    f = lambda x: len(set(x))\n",
    "    test = df.groupby([\"testId\"]).agg({\"suffix\": \"max\", \"KnowledgeTag\": f})\n",
    "    test.reset_index(inplace=True)\n",
    "\n",
    "    test.columns = [\"testId\", \"suffix_count\", \"tag_count\"]\n",
    "\n",
    "    df = pd.merge(df, test, on=\"testId\", how=\"left\")\n",
    "\n",
    "    gdf = df[[\"userID\", \"testId\", \"suffix\", \"prefix\", \"Timestamp\"]].sort_values(\n",
    "        by=[\"userID\", \"prefix\", \"Timestamp\"]\n",
    "    )\n",
    "    gdf[\"buserID\"] = gdf[\"userID\"] != gdf[\"userID\"].shift(1)\n",
    "    gdf[\"bprefix\"] = gdf[\"prefix\"] != gdf[\"prefix\"].shift(1)\n",
    "    gdf[\"first\"] = gdf[[\"buserID\", \"prefix\"]].any(axis=1).apply(lambda x: 1 - int(x))\n",
    "    gdf[\"RepeatedTime\"] = gdf[\"Timestamp\"].diff().fillna(pd.Timedelta(seconds=0))\n",
    "    gdf[\"RepeatedTime\"] = (\n",
    "        gdf[\"RepeatedTime\"].apply(lambda x: x.total_seconds()) * gdf[\"first\"]\n",
    "    )\n",
    "    df[\"RepeatedTime\"] = gdf[\"RepeatedTime\"].apply(lambda x: math.log(x + 1))\n",
    "    df[\"prior_KnowledgeTag_frequency\"] = df.groupby([\"userID\", \"KnowledgeTag\"]).cumcount()\n",
    "    df[\"testId\"] = df[\"testId\"].apply(lambda x: int(x[1:4] + x[-3]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.fillna(0) # null 값 분포 우선 fillna로 처리\n",
    "train.to_csv(\"/opt/ml/input/data/FE_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('DKT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fea09555e533e26ee8ea4f3cff5e1f079edf38fbdad54f1cdb58b91cfd061ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
